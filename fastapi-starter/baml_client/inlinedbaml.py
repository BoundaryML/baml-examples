###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off
import json

file_map = {
    
    "main.baml": "generator lang_python {\n  output_type \"python/pydantic\"\n  output_dir \"../\"\n}    ",
    "extract_resume.baml": "class Resume {\n  name string\n  education Education[]\n  skills string[]\n}\n\nclass Education {\n  school string\n  degree string\n  year int\n} \n \n  \nfunction ExtractResume(raw_text: string) -> Resume {\n  client GPT4\n  prompt #\"\n    Parse the following resume and return a structured representation of the data in the schema below.\n\n    Resume:\n    ---\n    {{raw_text}}\n    ---\n\n    Output JSON format (only include these fields, and no others):\n    {{ ctx.output_format(prefix=null) }}\n\n    Output JSON:\n  \"#\n}\n\ntest TestName {\n  functions [ExtractResume]\n  args {\n    raw_text #\"\n      John Doe\n      Education\n      ---\n      University of California, Berkeley\n      B.S. in Computer Science\n      2010\n      ---\n      Skills\n      ---\n      Java\n      Python\n      C++\n      ---\n    \"#\n  }\n}\n",
    "describe_image.baml": "",
    "classify_message.baml": "enum Category {\n    Refund\n    CancelOrder\n    TechnicalSupport\n    AccountIssue\n    Question\n}\n \nclass Message {\n  role Role\n  content string\n}\n\nenum Role {\n  Customer\n  Assistant\n}\n  \n  \ntemplate_string PrintMessage(msg: Message, prefix: string?) #\"\n  {{ _.chat('user' if msg.role == \"Customer\" else 'assistant') }}\n  {% if prefix %}\n  {{ prefix }}\n  {% endif %}\n  {{ msg.content }} \n\"# \n \nfunction ClassifyMessage(convo: Message[]) -> Category[] {\n  client GPT4\n  prompt #\"\n    {# \n      Prompts are auto-dedented and trimmed.\n      We use JINJA for our prompt syntax\n      (but we added some static analysis to make sure it's valid!)\n    #}\n\n    {{ ctx.output_format(prefix=\"Classify with the following json:\") }}\n\n    {% for c in convo %}\n    {{ PrintMessage(c, \n      'This is the message to classify:' if loop.last and convo|length > 1 else null\n    ) }}\n    {% endfor %}\n\n    {{ _.chat('assistant') }}\n    JSON array of categories that match:\n  \"#\n}\n",
    "clients.baml": "client<llm> GPT4 {\n  provider baml-openai-chat\n  options {\n    model gpt-4\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> GPT4Turbo {\n  provider baml-openai-chat\n  options {\n    model gpt-4-1106-preview\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> GPT3 {\n  provider baml-openai-chat\n  options {\n    model gpt-3.5-turbo\n    api_key env.OPENAI_API_KEY\n  }\n} ",
}

def get_baml_files():
    return file_map